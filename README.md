# ğŸ‘— Fashion CLIP Inference System  
A fully containerized, production-ready image embedding & fashion recommendation service powered by **OpenAI CLIP**, **TorchServe**, **FastAPI**, **Streamlit**, **Annoy (Spotify)**, and **Google Cloud Run**.

This project provides an end-to-end pipeline for generating vector embeddings from fashion images using CLIP, storing them in an approximate nearest neighbors index (Annoy), exposing them via an API, and serving results through a frontend UI.  
The system is cloud-native, serverless, fully containerized, and ready for production-scale inference.

---

## ğŸš€ Features

### ğŸ” Image Embedding with CLIP
- Uses **OpenAI CLIP ViT-B/32**  
- Automatically downloads and packages the model  
- Generates a TorchServe-ready `.mar` file using safe serialization (`safetensors`)

### ğŸ§  ANN Recommendations Using Annoy (Spotify)
- Fast approximate nearest neighbor search  
- Index rebuilt and persisted offline  
- Real-time querying for image similarity  
- Suitable for millions of vectors with minimal memory  
- Cloud Runâ€“friendly: runs purely in-memory without GPU requirements

### ğŸ–¥ FastAPI Backend
- Wraps TorchServe predictions  
- Exposes `/predict` or `/recommend` endpoints  
- Loads Annoy index for fast similarity search  
- Returns nearest-neighbor items using CLIP embeddings

### ğŸ¨ Streamlit Frontend
- Image upload  
- API interaction  
- Displays predicted embedding + recommended similar items  
- Minimal, clean UI

### â˜ Cloud-Native Architecture
- Three independent Cloud Run services:
  - `model-server` (TorchServe)
  - `api` (FastAPI + Annoy index)
  - `frontend` (Streamlit)
- Fully serverless  
- Auto-scalable  
- Built with Cloud Build

### ğŸ”§ Simple Model Installation
A standalone script `model_installer.py` downloads CLIP, packages it as `clip.mar`, and places it into:

model-server/model-store/clip.mar

This avoids buckets, gsutil, or any complex tooling.

---

## ğŸ— Project Structure

fashion_CloudRun/  
â”‚  
â”œâ”€â”€ model_installer.py            # Downloads CLIP + creates .mar file  
â”œâ”€â”€ build_annoy_index.py          # Builds Annoy similarity index  
â”œâ”€â”€ install_model.ipynb           # Notebook alternative  
â”œâ”€â”€ README.md  
â”‚  
â”œâ”€â”€ model-server/  
â”‚   â”œâ”€â”€ Dockerfile  
â”‚   â”œâ”€â”€ handler.py  
â”‚   â”œâ”€â”€ config.properties  
â”‚   â””â”€â”€ model-store/  
â”‚       â””â”€â”€ clip.mar  
â”‚  
â”œâ”€â”€ api/  
â”‚   â”œâ”€â”€ Dockerfile  
â”‚   â”œâ”€â”€ main.py  
â”‚   â”œâ”€â”€ annoy_index.ann            
â”‚  
â””â”€â”€ frontend/  
    â”œâ”€â”€ Dockerfile  
    â”œâ”€â”€ app.py  
  
---  

## ğŸ”§ Local Setup

### 1. Clone the repository
git clone <your-repo-url>
cd fashion_CloudRun

### 2. Install the CLIP model and create `.mar`
python3 model_installer.py

This generates:
model-server/model-store/clip.mar

## 3. Use your Cloud Run to run the project

---

## ğŸ”„ End-to-End Workflow

1. User uploads an image via Streamlit  
2. Frontend sends the image to FastAPI  
3. FastAPI calls TorchServe to compute CLIP embedding  
4. Annoy index returns similar items instantly  
5. Results displayed to the user  

---

## ğŸ§  Annoy Recommendation Engine Details

- Uses cosine similarity on CLIP embeddings  
- Persistent `.ann` index file    
- Loads extremely fast on Cloud Run (few MB)  
- Ideal for serverless inference scenarios

---

## âš™ Recommended Cloud Run Settings

Setting        | Value      
----------------|------------
Memory         | **1â€“2 GiB** 
CPU            | 1 vCPU     
Concurrency    | **1**      
Min Instances  | 0 or 1     
Max Instances  | 1â€“3        

TorchServe + CLIP requires >600 MB RAM, so Cloud Run default 512 MiB will fail.

---

## ğŸ›  Tech Stack

- CLIP ViT-B/32  
- TorchServe  
- FastAPI  
- Streamlit  
- Annoy (Spotify) for recommendations  
- Docker / Cloud Build  
- Google Cloud Run  

---

## ğŸ™Œ About

This project demonstrates a fully cloud-native ML inference pipeline, combining CLIP embeddings with an efficient approximate nearest neighbor search system (Annoy) to build real-time image similarity recommendations.  
