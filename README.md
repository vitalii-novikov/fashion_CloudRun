# ğŸ‘— Fashion CLIP Inference System  
A fully containerized, production-ready image embedding & fashion recommendation service powered by **OpenAI CLIP**, **TorchServe**, **FastAPI**, **Streamlit**, and **Google Cloud Run**.

This project provides an end-to-end pipeline for generating vector embeddings from fashion images using CLIP, wrapping them into an API, and serving them via a lightweight frontend. Everything is designed for cloud-native, scalable, serverless deployment.

---

## ğŸš€ Features

### ğŸ” Image Embedding with CLIP
- Uses **OpenAI CLIP ViT-B/32**  
- Automatically downloads and packages the model  
- Generates a TorchServe-ready `.mar` file using safe serialization (`safetensors`)

### ğŸ–¥ FastAPI Backend
- Wraps TorchServe predictions into a clean REST API  
- Environment-configurable (local & Cloud Run)

### ğŸ¨ Streamlit Frontend
- User uploads an image  
- Calls the API to generate embeddings  
- Fully Cloud Runâ€“compatible  

### â˜ Cloud-Native Architecture
- Each service runs independently on **Google Cloud Run**  
- Auto-scalable serverless deployment  
- Built via `gcloud builds submit`

### ğŸ”§ Simple Model Installation
A standalone script `model_installer.py` downloads the CLIP model, exports it as `clip.mar`, and saves it directly into:

model-server/model-store/clip.mar

No buckets, no gsutil, no startup scripts.

---

## ğŸ— Project Structure

fashion_CloudRun/
â”‚
â”œâ”€â”€ model_installer.py        # Downloads CLIP + creates .mar file
â”œâ”€â”€ install_model.ipynb       # Jupyter notebook alternative
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ model-server/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ handler.py
â”‚   â”œâ”€â”€ config.properties
â”‚   â””â”€â”€ model-store/
â”‚       â””â”€â”€ clip.mar
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ app.py
    â””â”€â”€ ...

---

## ğŸ”§ Local Setup

### 1. Clone the repository
git clone <your-repo-url>
cd fashion_CloudRun

### 2. Install the CLIP model and create `.mar`
python3 model_installer.py

This generates:
model-server/model-store/clip.mar

---

## ğŸ³ Build Docker Images via Cloud Build

### Build model server (TorchServe)
gcloud builds submit model-server --tag gcr.io/$GOOGLE_CLOUD_PROJECT/model-server

### Build API
gcloud builds submit api --tag gcr.io/$GOOGLE_CLOUD_PROJECT/api

### Build frontend
gcloud builds submit frontend --tag gcr.io/$GOOGLE_CLOUD_PROJECT/frontend

---

## â˜ï¸ Deploy to Google Cloud Run

### 1. Deploy TorchServe model server
gcloud run deploy model-server \
  --image gcr.io/$GOOGLE_CLOUD_PROJECT/model-server \
  --memory=2Gi \
  --concurrency=1 \
  --allow-unauthenticated \
  --region=europe-west3

### 2. Deploy API
gcloud run deploy api \
  --image gcr.io/$GOOGLE_CLOUD_PROJECT/api \
  --set-env-vars TORCHSERVE_URL=https://<model-server-url>/predictions/clip \
  --allow-unauthenticated \
  --region=europe-west3

### 3. Deploy frontend
gcloud run deploy frontend \
  --image gcr.io/$GOOGLE_CLOUD_PROJECT/frontend \
  --set-env-vars API_URL=https://<api-url> \
  --allow-unauthenticated \
  --region=europe-west3

---

## ğŸŒ Environment Variables

Each component supports a `.env` file:

TORCHSERVE_URL=https://model-server-xxxxx.run.app/predictions/clip
API_URL=https://api-xxxxx.run.app

---

## ğŸ”„ End-to-End Workflow

1. User uploads an image via Streamlit  
2. Frontend sends the image to FastAPI  
3. FastAPI forwards the request to TorchServe  
4. TorchServe runs CLIP ViT-B/32 and returns embeddings  
5. Frontend displays results  

---

## ğŸ§ª Testing the model server manually

curl -X POST \
  -F "data=@test.jpg" \
  https://model-server-xxxxx.run.app/predictions/clip

---

## âš™ Recommended Cloud Run Settings

Setting        | Value      
----------------|------------
Memory         | **1â€“2 GiB** 
CPU            | 1 vCPU     
Concurrency    | **1**      
Min Instances  | 0 or 1     
Max Instances  | 1â€“3        

TorchServe + CLIP requires >600 MB RAM.  
Cloud Run default 512 MiB will fail.

---

## ğŸ›  Tech Stack

- CLIP ViT-B/32  
- TorchServe  
- FastAPI  
- Streamlit  
- Docker / Cloud Build  
- Google Cloud Run  

---

## ğŸ™Œ About

This project demonstrates a complete cloud-native ML inference pipeline, including model packaging, scalable architecture, REST API serving, and frontend integration. Designed as a portfolio-grade example of deploying modern ML models in production.
