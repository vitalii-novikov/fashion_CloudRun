{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf39ac4",
   "metadata": {},
   "source": [
    "# Build CLIP/SigLIP model and upload .mar to Google Cloud Storage\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Downloads model from HuggingFace  \n",
    "2. Saves model in *safe* format (safetensors)  \n",
    "3. Creates `clip.mar`  \n",
    "4. Uploads to your GCS bucket inside `model-store/clip.mar`  \n",
    "\n",
    "You will then use this file in Cloud Run TorchServe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f890bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers safetensors google-cloud-storage torch-model-archiver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5241c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"your-bucket-name\"   # <-- change this\n",
    "MODEL_NAME = \"clip-vit-base-patch32\"   # or \"google/siglip-base\"\n",
    "EXPORT_DIR = \"clip-model-safe\"\n",
    "MODEL_STORE_DIR = \"model-store\"\n",
    "\n",
    "import os\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_STORE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "print(\"Downloading model:\", MODEL_NAME)\n",
    "\n",
    "model = CLIPModel.from_pretrained(f\"openai/{MODEL_NAME}\")\n",
    "processor = CLIPProcessor.from_pretrained(f\"openai/{MODEL_NAME}\")\n",
    "\n",
    "# ensure contiguous tensors (TorchServe requirement)\n",
    "for name, param in model.named_parameters():\n",
    "    param.data = param.data.contiguous()\n",
    "\n",
    "model.save_pretrained(EXPORT_DIR, safe_serialization=True)\n",
    "processor.save_pretrained(EXPORT_DIR)\n",
    "\n",
    "print(\"Model saved to\", EXPORT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver \\\n",
    "  --model-name clip \\\n",
    "  --version 1.0 \\\n",
    "  --serialized-file {EXPORT_DIR}/model.safetensors \\\n",
    "  --handler handler.py \\\n",
    "  --extra-files \"{EXPORT_DIR}/config.json,{EXPORT_DIR}/preprocessor_config.json,{EXPORT_DIR}/tokenizer_config.json,{EXPORT_DIR}/merges.txt,{EXPORT_DIR}/vocab.json,{EXPORT_DIR}/special_tokens_map.json\" \\\n",
    "  --export-path {MODEL_STORE_DIR} \\\n",
    "  --force\n",
    "\n",
    "print(\"MAR created at:\", f\"{MODEL_STORE_DIR}/clip.mar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda88f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "blob = bucket.blob(\"model-store/clip.mar\")\n",
    "blob.upload_from_filename(f\"{MODEL_STORE_DIR}/clip.mar\")\n",
    "\n",
    "print(\"Uploaded to:\")\n",
    "print(f\"gs://{BUCKET_NAME}/model-store/clip.mar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Use these ENV variables in Cloud Run services:\")\n",
    "\n",
    "print(\"\\nTORCHSERVE_URL:\")\n",
    "print(\"https://<model-server-service>.run.app/predictions/clip\")\n",
    "\n",
    "print(\"\\nMODEL_MAR_GCS_PATH:\")\n",
    "print(f\"gs://{BUCKET_NAME}/model-store/clip.mar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac36ac5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
